{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2WIsojsY7JT3u77weEQ8L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vimala-Suram/Agents/blob/main/Assign_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This notebook implements a research agent that uses LLMs to:\n",
        "1. Break down a topic into subtopics\n",
        "2. Expand queries with relevant terms\n",
        "3. Search the web for information\n",
        "4. Generate and improve summaries\n",
        "5. Critique and reflect on the research\n",
        "\n",
        "It uses Together AI's API to access various LLM models, with safeguards\n",
        "to minimize API costs through caching and mock modes for development.\n",
        "\"\"\"\n",
        "\n",
        "# ## Setup and Imports\n",
        "\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from typing import List, Dict, Any, Optional, Callable, Tuple\n",
        "from functools import lru_cache\n",
        "import hashlib\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# !pip install openai requests python-dotenv\n",
        "\n",
        "# Import environment variables from .env file (optional but recommended)\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()  # Load environment variables from .env file\n",
        "    print(\"Environment variables loaded from .env file\")\n",
        "except ImportError:\n",
        "    print(\"python-dotenv not installed, skipping .env file loading\")\n",
        "\n",
        "# Configure the client for Together AI using the v1.0+ OpenAI API format\n",
        "from openai import OpenAI\n",
        "\n",
        "# For Colab: Use secrets to avoid exposing API keys\n",
        "# Uncomment the following lines if using Google Colab\n",
        "# from google.colab import userdata\n",
        "# api_key = userdata.get('TOGETHER_API_KEY')\n",
        "\n",
        "# For local development: Use environment variables\n",
        "api_key = os.environ.get(\"TOGETHER_API_KEY\")\n",
        "if not api_key:\n",
        "    api_key = userdata.get('TOGETHER_API_KEY')\n",
        "    os.environ[\"TOGETHER_API_KEY\"] = api_key\n",
        "\n",
        "# Create the OpenAI client with Together AI base URL\n",
        "client = OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"https://api.together.xyz/v1\"\n",
        ")\n",
        "\n",
        "# Set default model (you can change this based on your preference)\n",
        "# Note: Smaller models are cheaper but may provide lower quality results\n",
        "DEFAULT_MODEL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"  # Good balance of performance and cost\n",
        "# Other options:\n",
        "# - \"togethercomputer/llama-2-7b-chat\" - Lighter model, faster responses\n",
        "# - \"meta-llama/Meta-Llama-3-8B-Instruct\" - Another good option\n",
        "# - \"meta-llama/Llama-2-70b-chat-hf\" - Larger but more expensive\n",
        "\n",
        "# Flag to use mock search during development to avoid API costs\n",
        "USE_MOCK_SEARCH = True  # Set to False when ready to use real search API\n",
        "\n",
        "print(f\"Using model: {DEFAULT_MODEL}\")\n",
        "print(f\"Mock search mode: {'Enabled for development purposes only' if USE_MOCK_SEARCH else 'Disabled - will use real search API'}\")\n",
        "\n",
        "# ## Tool 1: Topic Breakdown Tool\n",
        "\n",
        "def topic_breakdown_tool(topic: str, num_subtopics: int = 3, model: str = DEFAULT_MODEL) -> List[str]:\n",
        "    \"\"\"\n",
        "    Breaks down a broad research topic into smaller, more focused subtopics.\n",
        "\n",
        "    Args:\n",
        "        topic: The main research topic\n",
        "        num_subtopics: Number of subtopics to generate\n",
        "        model: LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        List of subtopics\n",
        "    \"\"\"\n",
        "    # Create a prompt for the LLM\n",
        "    system_prompt = \"You are a research assistant that helps break down topics into focused subtopics.\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    I need to research the topic: \"{topic}\"\n",
        "\n",
        "    Please break this broad topic down into {num_subtopics} specific, focused subtopics that would be useful for research.\n",
        "    For each subtopic, provide a clear, concise phrase that could be used as a search query.\n",
        "\n",
        "    Return ONLY the list of subtopics, one per line, without any additional text, numbering, or explanations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the LLM using the new API format\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "        temperature=0.3  # Lower temperature for more focused results\n",
        "    )\n",
        "\n",
        "    # Extract and clean the subtopics\n",
        "    subtopics_text = response.choices[0].message.content.strip()\n",
        "    subtopics = [line.strip() for line in subtopics_text.split('\\n') if line.strip()]\n",
        "\n",
        "    # Ensure we have the requested number of subtopics\n",
        "    if len(subtopics) < num_subtopics:\n",
        "        # If we don't have enough, make another call to get more\n",
        "        print(f\"Only got {len(subtopics)} subtopics, requesting more...\")\n",
        "\n",
        "        more_subtopics_prompt = f\"\"\"\n",
        "        I need additional subtopics related to: \"{topic}\"\n",
        "\n",
        "        I already have these subtopics:\n",
        "        {subtopics_text}\n",
        "\n",
        "        Please provide {num_subtopics - len(subtopics)} more specific, different subtopics.\n",
        "        Return ONLY the list of additional subtopics, one per line.\n",
        "        \"\"\"\n",
        "\n",
        "        more_response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": more_subtopics_prompt}\n",
        "            ],\n",
        "            max_tokens=300,\n",
        "            temperature=0.4  # Slightly higher temperature for more diverse results\n",
        "        )\n",
        "\n",
        "        more_subtopics = [line.strip() for line in more_response.choices[0].message.content.strip().split('\\n') if line.strip()]\n",
        "        subtopics.extend(more_subtopics)\n",
        "\n",
        "    return subtopics[:num_subtopics]  # Return exactly the requested number\n",
        "\n",
        "# ## Tool 2: Query Expansion Tool\n",
        "\n",
        "def query_expansion_tool(query: str, num_expansions: int = 3, model: str = DEFAULT_MODEL) -> List[str]:\n",
        "    \"\"\"\n",
        "    Expands a query with related keywords, synonyms, and phrases.\n",
        "\n",
        "    Args:\n",
        "        query: The original search query\n",
        "        num_expansions: Number of expanded queries to generate\n",
        "        model: LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        List of expanded queries\n",
        "    \"\"\"\n",
        "    system_prompt = \"You are a search query optimization specialist.\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    Original search query: \"{query}\"\n",
        "\n",
        "    Please generate {num_expansions} expanded versions of this query by:\n",
        "    1. Adding relevant keywords\n",
        "    2. Using synonyms for key terms\n",
        "    3. Rephrasing to capture the same concept in different ways\n",
        "\n",
        "    Return ONLY the expanded queries, one per line, without any numbering or explanations.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "        temperature=0.4\n",
        "    )\n",
        "\n",
        "    expanded_queries_text = response.choices[0].message.content.strip()\n",
        "    expanded_queries = [line.strip() for line in expanded_queries_text.split('\\n') if line.strip()]\n",
        "\n",
        "    return expanded_queries[:num_expansions]  # Ensure we only get the requested number\n",
        "\n",
        "# ## Tool 3: Search Tool\n",
        "\n",
        "class MockSearchTool:\n",
        "    \"\"\"Mock search tool for development without using real API costs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.cache = {}\n",
        "\n",
        "    def search(self, query: str, num_results: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Generate mock search results for a query.\n",
        "\n",
        "        Args:\n",
        "            query: Search query\n",
        "            num_results: Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List of mock search results\n",
        "        \"\"\"\n",
        "        # Check if this query is already in the cache\n",
        "        cache_key = f\"{query}_{num_results}\"\n",
        "        if cache_key in self.cache:\n",
        "            print(f\"Using cached mock results for query: {query}\")\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Generate deterministic but diverse mock results based on query\n",
        "        query_hash = hashlib.md5(query.encode()).hexdigest()\n",
        "        results = []\n",
        "\n",
        "        for i in range(num_results):\n",
        "            seed = f\"{query_hash}_{i}\"\n",
        "            result_hash = hashlib.md5(seed.encode()).hexdigest()\n",
        "\n",
        "            # Use the hash to generate somewhat diverse mock content\n",
        "            words = query.split()\n",
        "            title_words = words + [\"research\", \"analysis\", \"overview\", \"studies\", \"applications\", \"advances\", \"technology\"]\n",
        "            snippet_words = words + [\"provides\", \"details\", \"explains\", \"examines\", \"discusses\", \"presents\", \"describes\", \"shows\"]\n",
        "\n",
        "            # Use the hash to seed a simple random choice\n",
        "            title_idx = [ord(c) % len(title_words) for c in result_hash[:5]]\n",
        "            snippet_idx = [ord(c) % len(snippet_words) for c in result_hash[5:10]]\n",
        "\n",
        "            title = f\"{' '.join([title_words[idx] for idx in title_idx])} - {query} ({i+1})\"\n",
        "            snippet = f\"This {snippet_words[snippet_idx[0]]} {query} and {snippet_words[snippet_idx[1]]} its importance. \"\n",
        "            snippet += f\"The article {snippet_words[snippet_idx[2]]} various aspects including {words[ord(result_hash[10]) % len(words)] if words else 'topics'}. \"\n",
        "            snippet += f\"Research {snippet_words[snippet_idx[3]]} that {words[ord(result_hash[11]) % len(words)] if words else 'this'} is significant for future developments.\"\n",
        "\n",
        "            results.append({\n",
        "                \"title\": title.title(),\n",
        "                \"snippet\": snippet,\n",
        "                \"url\": f\"https://example.com/result/{result_hash[:8]}\",\n",
        "                \"source\": \"Mock Search Engine\"\n",
        "            })\n",
        "\n",
        "        # Cache the results\n",
        "        self.cache[cache_key] = results\n",
        "        return results\n",
        "\n",
        "class SerperSearchTool:\n",
        "    \"\"\"\n",
        "    Search tool using Serper.dev API (Google Search API)\n",
        "    Documentation: https://serper.dev/docs\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str = None):\n",
        "        self.api_key = api_key or os.environ.get(\"SERPER_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"Serper API key is required\")\n",
        "\n",
        "        self.base_url = \"https://google.serper.dev/search\"\n",
        "        self.headers = {\n",
        "            \"X-API-KEY\": self.api_key,\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        # Cache to minimize API calls\n",
        "        self.cache = {}\n",
        "\n",
        "    @lru_cache(maxsize=100)\n",
        "    def search(self, query: str, num_results: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Search the web using Serper.dev API.\n",
        "\n",
        "        Args:\n",
        "            query: The search query\n",
        "            num_results: Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List of search results\n",
        "        \"\"\"\n",
        "        # Check cache first\n",
        "        cache_key = f\"{query}_{num_results}\"\n",
        "        if cache_key in self.cache:\n",
        "            print(f\"Using cached results for query: {query}\")\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        payload = {\n",
        "            \"q\": query,\n",
        "            \"num\": num_results\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(self.base_url, headers=self.headers, json=payload)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            data = response.json()\n",
        "\n",
        "            # Format the results\n",
        "            formatted_results = []\n",
        "            if \"organic\" in data:\n",
        "                for item in data[\"organic\"][:num_results]:\n",
        "                    formatted_results.append({\n",
        "                        \"title\": item.get(\"title\", \"No title\"),\n",
        "                        \"snippet\": item.get(\"snippet\", \"No snippet available\"),\n",
        "                        \"url\": item.get(\"link\", \"\"),\n",
        "                        \"source\": f\"Serper.dev: {item.get('link', '')}\"\n",
        "                    })\n",
        "\n",
        "            # Cache the results\n",
        "            self.cache[cache_key] = formatted_results\n",
        "            return formatted_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in search_tool: {e}\")\n",
        "            return [{\n",
        "                \"title\": \"Error\",\n",
        "                \"snippet\": f\"An error occurred: {str(e)}\",\n",
        "                \"url\": \"\",\n",
        "                \"source\": \"Error\"\n",
        "            }]\n",
        "\n",
        "class BraveSearchTool:\n",
        "    \"\"\"\n",
        "    Search tool using Brave Search API\n",
        "    Documentation: https://brave.com/search/api/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str = None):\n",
        "        self.api_key = api_key or os.environ.get(\"BRAVE_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"Brave Search API key is required\")\n",
        "\n",
        "        self.base_url = \"https://api.search.brave.com/res/v1/web/search\"\n",
        "        self.headers = {\n",
        "            \"Accept\": \"application/json\",\n",
        "            \"Accept-Encoding\": \"gzip\",\n",
        "            \"X-Subscription-Token\": self.api_key\n",
        "        }\n",
        "\n",
        "        # Cache to minimize API calls\n",
        "        self.cache = {}\n",
        "\n",
        "    @lru_cache(maxsize=100)\n",
        "    def search(self, query: str, num_results: int = 5, country: str = \"US\") -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Search the web using Brave Search API.\n",
        "\n",
        "        Args:\n",
        "            query: The search query\n",
        "            num_results: Number of results to return\n",
        "            country: Country code for localized results\n",
        "\n",
        "        Returns:\n",
        "            List of search results\n",
        "        \"\"\"\n",
        "        # Check cache first\n",
        "        cache_key = f\"{query}_{num_results}_{country}\"\n",
        "        if cache_key in self.cache:\n",
        "            print(f\"Using cached results for query: {query}\")\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"count\": min(num_results, 20),  # API limit is 20\n",
        "            \"country\": country,\n",
        "            \"search_lang\": \"en\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(self.base_url, headers=self.headers, params=params)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            data = response.json()\n",
        "\n",
        "            # Format the results\n",
        "            formatted_results = []\n",
        "            if \"web\" in data and \"results\" in data[\"web\"]:\n",
        "                for result in data[\"web\"][\"results\"]:\n",
        "                    formatted_results.append({\n",
        "                        \"title\": result.get(\"title\", \"No title\"),\n",
        "                        \"snippet\": result.get(\"description\", \"No description available\"),\n",
        "                        \"url\": result.get(\"url\", \"\"),\n",
        "                        \"source\": f\"Brave Search: {result.get('url', '')}\"\n",
        "                    })\n",
        "\n",
        "            # Cache the results\n",
        "            self.cache[cache_key] = formatted_results\n",
        "            return formatted_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in search_tool: {e}\")\n",
        "            return [{\n",
        "                \"title\": \"Error\",\n",
        "                \"snippet\": f\"An error occurred: {str(e)}\",\n",
        "                \"url\": \"\",\n",
        "                \"source\": \"Error\"\n",
        "            }]\n",
        "\n",
        "# Create the appropriate search tool based on configuration\n",
        "def get_search_tool():\n",
        "    \"\"\"\n",
        "    Factory function to create the appropriate search tool\n",
        "    based on configuration and available API keys.\n",
        "    \"\"\"\n",
        "    if USE_MOCK_SEARCH:\n",
        "        print(\"Using mock search tool\")\n",
        "        return MockSearchTool()\n",
        "\n",
        "    # Try to use Brave Search if API key is available\n",
        "    brave_api_key = os.environ.get(\"BRAVE_API_KEY\")\n",
        "    if brave_api_key:\n",
        "        print(\"Using Brave Search API\")\n",
        "        return BraveSearchTool(api_key=brave_api_key)\n",
        "\n",
        "    # Try to use Serper if API key is available\n",
        "    serper_api_key = os.environ.get(\"SERPER_API_KEY\")\n",
        "    if serper_api_key:\n",
        "        print(\"Using Serper.dev API\")\n",
        "        return SerperSearchTool(api_key=serper_api_key)\n",
        "\n",
        "    # Fall back to mock search if no API keys are available\n",
        "    print(\"No search API keys found, falling back to mock search\")\n",
        "    return MockSearchTool()\n",
        "\n",
        "# Create the search tool\n",
        "search_tool_instance = get_search_tool()\n",
        "\n",
        "def search_tool(query: str, num_results: int = 3) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Wrapper function to use the appropriate search tool.\n",
        "\n",
        "    Args:\n",
        "        query: Search query\n",
        "        num_results: Number of results to return\n",
        "\n",
        "    Returns:\n",
        "        List of search results\n",
        "    \"\"\"\n",
        "    return search_tool_instance.search(query, num_results)\n",
        "\n",
        "# ## Tool 4: Critique Tool\n",
        "\n",
        "def critique_tool(summary: str, topic: str, model: str = DEFAULT_MODEL) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Critiques a research summary and suggests improvements.\n",
        "\n",
        "    Args:\n",
        "        summary: The summary to critique\n",
        "        topic: The original research topic\n",
        "        model: LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with critique and suggestions\n",
        "    \"\"\"\n",
        "    system_prompt = \"You are a research critic who provides constructive feedback on summaries.\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    Research Topic: \"{topic}\"\n",
        "\n",
        "    Summary to Review:\n",
        "    \"{summary}\"\n",
        "\n",
        "    Please critique this research summary and provide feedback on:\n",
        "    1. Accuracy and relevance to the topic\n",
        "    2. Completeness - are important aspects missing?\n",
        "    3. Clarity and organization\n",
        "    4. Potential biases or limitations\n",
        "    5. Specific suggestions for improvement\n",
        "    6. Additional subtopics that should be explored\n",
        "\n",
        "    Format your response as a JSON object with these keys:\n",
        "    - strengths: list of strengths (2-3 items)\n",
        "    - weaknesses: list of weaknesses (2-3 items)\n",
        "    - suggestions: list of specific suggestions for improvement (2-3 items)\n",
        "    - additional_topics: list of additional subtopics to explore (1-2 items)\n",
        "\n",
        "    Ensure your response is valid JSON.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        max_tokens=500,\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    critique_text = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Parsing the JSON response\n",
        "    try:\n",
        "        critique = json.loads(critique_text)\n",
        "    except json.JSONDecodeError:\n",
        "        # If parsing fails creating a simple structure with the raw text\n",
        "        print(\"Warning: Failed to parse critique as JSON. Using raw text instead.\")\n",
        "        critique = {\n",
        "            \"strengths\": [\"Summary provides relevant information\"],\n",
        "            \"weaknesses\": [\"Summary could be improved\"],\n",
        "            \"suggestions\": [\"Consider expanding on key points\"],\n",
        "            \"additional_topics\": [\"Related subtopics\"],\n",
        "            \"raw_critique\": critique_text\n",
        "        }\n",
        "\n",
        "    return critique\n",
        "\n",
        "# ## Tool 5: Summarizer Tool\n",
        "\n",
        "def summarizer_tool(content: List[Dict[str, str]], topic: str, model: str = DEFAULT_MODEL) -> str:\n",
        "    \"\"\"\n",
        "    Summarizes content from search results.\n",
        "\n",
        "    Args:\n",
        "        content: List of dictionaries with search results\n",
        "        topic: The original research topic\n",
        "        model: LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        A concise summary paragraph\n",
        "    \"\"\"\n",
        "    # Format the search results into a readable text\n",
        "    formatted_content = \"\\n\\n\".join([\n",
        "        f\"Title: {item['title']}\\nSnippet: {item['snippet']}\\nURL: {item['url']}\"\n",
        "        for item in content\n",
        "    ])\n",
        "\n",
        "    system_prompt = \"You are a research assistant that creates concise, accurate summaries.\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    Research Topic: \"{topic}\"\n",
        "\n",
        "    Below are search results about this topic:\n",
        "\n",
        "    {formatted_content}\n",
        "\n",
        "    Please synthesize this information into a concise, informative summary paragraph about the topic.\n",
        "    Focus on key findings, common themes, and important details.\n",
        "    Keep your summary to approximately 4-6 sentences.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        max_tokens=400,\n",
        "        temperature=0.4\n",
        "    )\n",
        "\n",
        "    summary = response.choices[0].message.content.strip()\n",
        "    return summary\n",
        "\n",
        "# ## Research Agent Implementation\n",
        "\n",
        "class ResearchAgent:\n",
        "    \"\"\"\n",
        "    An agent that uses LLMs and tools to research topics and generate summaries.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: str = DEFAULT_MODEL):\n",
        "        \"\"\"\n",
        "        Initialize the research agent with its tools and model.\n",
        "\n",
        "        Args:\n",
        "            model: LLM model to use\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        print(f\"Initializing ResearchAgent with model: {model}\")\n",
        "\n",
        "        # Initialize the state\n",
        "        self.state = {\n",
        "            \"main_topic\": \"\",\n",
        "            \"subtopics\": [],\n",
        "            \"expanded_queries\": {},\n",
        "            \"search_results\": {},\n",
        "            \"current_summary\": \"\",\n",
        "            \"critique\": {},\n",
        "            \"final_summary\": \"\",\n",
        "            \"steps_taken\": []\n",
        "        }\n",
        "\n",
        "    def _log_step(self, step_name: str, input_params: Dict[str, Any], output: Any):\n",
        "        \"\"\"Log a step in the research process\"\"\"\n",
        "        self.state[\"steps_taken\"].append({\n",
        "            \"step\": step_name,\n",
        "            \"timestamp\": time.time(),\n",
        "            \"input\": input_params,\n",
        "            \"output_summary\": str(output)[:100] + \"...\" if isinstance(output, str) and len(str(output)) > 100 else str(output)\n",
        "        })\n",
        "\n",
        "        print(f\"\\n--- Step: {step_name} ---\")\n",
        "        print(f\"Input: {input_params}\")\n",
        "        if isinstance(output, str):\n",
        "            print(f\"Output: {output[:100]}...\" if len(output) > 100 else output)\n",
        "        elif isinstance(output, list):\n",
        "            print(f\"Output: {len(output)} items\")\n",
        "            for i, item in enumerate(output[:3]):\n",
        "                print(f\"  {i+1}. {str(item)[:50]}...\")\n",
        "            if len(output) > 3:\n",
        "                print(f\"  ... and {len(output) - 3} more\")\n",
        "        elif isinstance(output, dict):\n",
        "            print(f\"Output: Dictionary with keys {list(output.keys())}\")\n",
        "        else:\n",
        "            print(f\"Output: {type(output)}\")\n",
        "\n",
        "    def research(self, topic: str, num_subtopics: int = 3, verbose: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Perform research on a topic and generate a summary.\n",
        "\n",
        "        Args:\n",
        "            topic: The research topic\n",
        "            num_subtopics: Number of subtopics to explore\n",
        "            verbose: Whether to print progress information\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with research results\n",
        "        \"\"\"\n",
        "        # Initialize state for this research session\n",
        "        self.state = {\n",
        "            \"main_topic\": topic,\n",
        "            \"subtopics\": [],\n",
        "            \"expanded_queries\": {},\n",
        "            \"search_results\": {},\n",
        "            \"current_summary\": \"\",\n",
        "            \"critique\": {},\n",
        "            \"final_summary\": \"\",\n",
        "            \"steps_taken\": []\n",
        "        }\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Starting research on topic: {topic}\")\n",
        "\n",
        "        # Step 1: Break down the topic into subtopics\n",
        "        subtopics = topic_breakdown_tool(topic, num_subtopics, self.model)\n",
        "        self.state[\"subtopics\"] = subtopics\n",
        "        self._log_step(\"topic_breakdown\", {\"topic\": topic, \"num_subtopics\": num_subtopics}, subtopics)\n",
        "\n",
        "        # Step 2: Expand queries for the main topic and subtopics\n",
        "        # First expand the main topic\n",
        "        expanded_main_queries = query_expansion_tool(topic, 3, self.model)\n",
        "        self.state[\"expanded_queries\"][topic] = expanded_main_queries\n",
        "        self._log_step(\"query_expansion_main\", {\"query\": topic}, expanded_main_queries)\n",
        "\n",
        "        # Then expand each subtopic\n",
        "        for subtopic in subtopics:\n",
        "            expanded_queries = query_expansion_tool(subtopic, 2, self.model)\n",
        "            self.state[\"expanded_queries\"][subtopic] = expanded_queries\n",
        "            self._log_step(\"query_expansion_subtopic\", {\"query\": subtopic}, expanded_queries)\n",
        "\n",
        "        # Step 3: Search for information\n",
        "        # First, search the main topic using the first expanded query\n",
        "        main_query = self.state[\"expanded_queries\"][topic][0] if self.state[\"expanded_queries\"][topic] else topic\n",
        "        main_results = search_tool(main_query, 3)\n",
        "        self.state[\"search_results\"][topic] = main_results\n",
        "        self._log_step(\"search_main_topic\", {\"query\": main_query, \"num_results\": 3}, main_results)\n",
        "\n",
        "        # Then search for each subtopic\n",
        "        all_search_results = main_results.copy()\n",
        "        for subtopic in subtopics:\n",
        "            # Use the first expanded query if available\n",
        "            subtopic_query = self.state[\"expanded_queries\"][subtopic][0] if self.state[\"expanded_queries\"][subtopic] else subtopic\n",
        "            results = search_tool(subtopic_query, 2)\n",
        "            self.state[\"search_results\"][subtopic] = results\n",
        "            all_search_results.extend(results)\n",
        "            self._log_step(\"search_subtopic\", {\"query\": subtopic_query, \"subtopic\": subtopic, \"num_results\": 2}, results)\n",
        "\n",
        "        # Step 4: Generate an initial summary\n",
        "        initial_summary = summarizer_tool(all_search_results, topic, self.model)\n",
        "        self.state[\"current_summary\"] = initial_summary\n",
        "        self._log_step(\"generate_summary\", {\"topic\": topic, \"num_results\": len(all_search_results)}, initial_summary)\n",
        "\n",
        "        # Step 5: Critique the summary\n",
        "        critique = critique_tool(initial_summary, topic, self.model)\n",
        "        self.state[\"critique\"] = critique\n",
        "        self._log_step(\"critique_summary\", {\"summary\": initial_summary, \"topic\": topic}, critique)\n",
        "\n",
        "        # Step 6: Improve the summary based on the critique\n",
        "        # Create a prompt for improving the summary\n",
        "        system_prompt = \"You are a research assistant that improves summaries based on feedback.\"\n",
        "\n",
        "        improvement_prompt = f\"\"\"\n",
        "        Original Research Topic: \"{topic}\"\n",
        "\n",
        "        Original Summary:\n",
        "        \"{initial_summary}\"\n",
        "\n",
        "        Critique:\n",
        "        - Strengths: {', '.join(critique['strengths'])}\n",
        "        - Weaknesses: {', '.join(critique['weaknesses'])}\n",
        "        - Suggestions: {', '.join(critique['suggestions'])}\n",
        "\n",
        "        Please rewrite the summary to address these weaknesses and incorporate the suggestions.\n",
        "        Create an improved, comprehensive yet concise paragraph (4-6 sentences).\n",
        "        \"\"\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": improvement_prompt}\n",
        "            ],\n",
        "            max_tokens=400,\n",
        "            temperature=0.4\n",
        "        )\n",
        "\n",
        "        final_summary = response.choices[0].message.content.strip()\n",
        "        self.state[\"final_summary\"] = final_summary\n",
        "        self._log_step(\"improve_summary\", {\"original_summary\": initial_summary, \"critique\": critique, \"topic\": topic}, final_summary)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"RESEARCH COMPLETE\")\n",
        "            print(\"=\"*50)\n",
        "            print(f\"Topic: {topic}\")\n",
        "            print(\"\\nSubtopics explored:\")\n",
        "            for subtopic in subtopics:\n",
        "                print(f\"- {subtopic}\")\n",
        "            print(\"\\nFinal Summary:\")\n",
        "            print(final_summary)\n",
        "            print(\"=\"*50)\n",
        "\n",
        "        # Return the research results\n",
        "        return {\n",
        "            \"topic\": topic,\n",
        "            \"subtopics\": subtopics,\n",
        "            \"search_results\": self.state[\"search_results\"],\n",
        "            \"initial_summary\": initial_summary,\n",
        "            \"critique\": critique,\n",
        "            \"final_summary\": final_summary,\n",
        "            \"steps\": self.state[\"steps_taken\"]\n",
        "        }\n",
        "\n",
        "# Interactive function to run the agent\n",
        "def run_interactive():\n",
        "    \"\"\"Run the research agent interactively\"\"\"\n",
        "    print(\"\\n=== Research Agent ===\\n\")\n",
        "    topic = input(\"Enter a research topic: \")\n",
        "\n",
        "    num_subtopics = 3\n",
        "    try:\n",
        "        num_subtopics_input = input(\"Number of subtopics to explore (default: 3): \")\n",
        "        if num_subtopics_input.strip():\n",
        "            num_subtopics = int(num_subtopics_input)\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Using default value of 3 subtopics.\")\n",
        "\n",
        "    model = DEFAULT_MODEL\n",
        "    model_input = input(f\"LLM Model to use (default: {DEFAULT_MODEL}): \")\n",
        "    if model_input.strip():\n",
        "        model = model_input\n",
        "\n",
        "    print(f\"\\nResearching '{topic}' with {num_subtopics} subtopics using {model}...\\n\")\n",
        "\n",
        "    agent = ResearchAgent(model=model)\n",
        "    results = agent.research(topic, num_subtopics=num_subtopics, verbose=True)\n",
        "\n",
        "    # Ask if the user wants to see detailed results\n",
        "    show_details = input(\"\\nShow detailed research results? (y/n): \").lower() == 'y'\n",
        "\n",
        "    if show_details:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"DETAILED RESEARCH RESULTS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        print(\"SUBTOPICS:\")\n",
        "        for i, subtopic in enumerate(results[\"subtopics\"], 1):\n",
        "            print(f\"{i}. {subtopic}\")\n",
        "\n",
        "            # Show expanded queries for this subtopic\n",
        "            if subtopic in agent.state[\"expanded_queries\"]:\n",
        "                print(\"   Expanded queries:\")\n",
        "                for j, query in enumerate(agent.state[\"expanded_queries\"][subtopic], 1):\n",
        "                    print(f\"   {j}. {query}\")\n",
        "\n",
        "            # Show search results for this subtopic\n",
        "            if subtopic in results[\"search_results\"]:\n",
        "                print(\"   Search results:\")\n",
        "                for j, result in enumerate(results[\"search_results\"][subtopic], 1):\n",
        "                    print(f\"   {j}. {result['title']}\")\n",
        "                    print(f\"      {result['snippet'][:100]}...\")\n",
        "\n",
        "            print()\n",
        "\n",
        "        print(\"\\nINITIAL SUMMARY:\")\n",
        "        print(results[\"initial_summary\"])\n",
        "\n",
        "        print(\"\\nCRITIQUE:\")\n",
        "        print(\"Strengths:\")\n",
        "        for strength in results[\"critique\"][\"strengths\"]:\n",
        "            print(f\"- {strength}\")\n",
        "        print(\"Weaknesses:\")\n",
        "        for weakness in results[\"critique\"][\"weaknesses\"]:\n",
        "            print(f\"- {weakness}\")\n",
        "        print(\"Suggestions:\")\n",
        "        for suggestion in results[\"critique\"][\"suggestions\"]:\n",
        "            print(f\"- {suggestion}\")\n",
        "        if \"additional_topics\" in results[\"critique\"]:\n",
        "            print(\"Additional topics to explore:\")\n",
        "            for topic in results[\"critique\"][\"additional_topics\"]:\n",
        "                print(f\"- {topic}\")\n",
        "\n",
        "        print(\"\\nFINAL SUMMARY:\")\n",
        "        print(results[\"final_summary\"])\n",
        "\n",
        "        print(\"=\"*50)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_interactive()\n",
        "else:\n",
        "    print(\"Import complete. Use ResearchAgent class or run_interactive() function to start researching.\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Testing the Research Agent\n",
        "#\n",
        "# Let's test the agent with a sample topic:\n",
        "\n",
        "# %%\n",
        "# Uncomment to run a test\n",
        "# agent = ResearchAgent()\n",
        "# test_results = agent.research(\"Quantum computing applications in healthcare\", verbose=True)\n",
        "# print(test_results[\"final_summary\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMXrdwa3oMa2",
        "outputId": "a1feb6b5-1c33-4a40-f226-8f08a94ea5ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables loaded from .env file\n",
            "Using model: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
            "Mock search mode: Enabled for development purposes only\n",
            "Using mock search tool\n",
            "\n",
            "=== Research Agent ===\n",
            "\n",
            "Enter a research topic: Learning deep features for discriminative localization\n",
            "Number of subtopics to explore (default: 3): 2\n",
            "LLM Model to use (default: mistralai/Mixtral-8x7B-Instruct-v0.1): mistralai/Mixtral-8x7B-Instruct-v0.1\n",
            "\n",
            "Researching 'Learning deep features for discriminative localization' with 2 subtopics using mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
            "\n",
            "Initializing ResearchAgent with model: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
            "Starting research on topic: Learning deep features for discriminative localization\n",
            "\n",
            "--- Step: topic_breakdown ---\n",
            "Input: {'topic': 'Learning deep features for discriminative localization', 'num_subtopics': 2}\n",
            "Output: 2 items\n",
            "  1. \"Deep learning methods for feature extraction in d...\n",
            "  2. \"Comparison of deep feature-based approaches for d...\n",
            "\n",
            "--- Step: query_expansion_main ---\n",
            "Input: {'query': 'Learning deep features for discriminative localization'}\n",
            "Output: 3 items\n",
            "  1. * \"Deep feature learning for discriminative locali...\n",
            "  2. * \"Feature extraction for discriminative object lo...\n",
            "  3. * \"Utilizing deep features to achieve localization...\n",
            "\n",
            "--- Step: query_expansion_subtopic ---\n",
            "Input: {'query': '\"Deep learning methods for feature extraction in discriminative localization\"'}\n",
            "Output: 2 items\n",
            "  1. \"Deep learning techniques for feature extraction i...\n",
            "  2. \"Deep learning approaches for characteristic extra...\n",
            "\n",
            "--- Step: query_expansion_subtopic ---\n",
            "Input: {'query': '\"Comparison of deep feature-based approaches for discriminative object localization\"'}\n",
            "Output: 2 items\n",
            "  1. \"Comparative analysis of deep feature methods for ...\n",
            "  2. \"Deep feature-based approaches for accurate object...\n",
            "\n",
            "--- Step: search_main_topic ---\n",
            "Input: {'query': '* \"Deep feature learning for discriminative localization and classification\"', 'num_results': 3}\n",
            "Output: 3 items\n",
            "  1. {'title': 'Localization Discriminative Learning Fe...\n",
            "  2. {'title': 'Classification\" Localization And Learni...\n",
            "  3. {'title': 'Feature For \"Deep Discriminative For - ...\n",
            "\n",
            "--- Step: search_subtopic ---\n",
            "Input: {'query': '\"Deep learning techniques for feature extraction in discriminative landmark identification\"', 'subtopic': '\"Deep learning methods for feature extraction in discriminative localization\"', 'num_results': 2}\n",
            "Output: 2 items\n",
            "  1. {'title': 'Techniques Studies Technology Applicati...\n",
            "  2. {'title': 'Techniques Advances Extraction Techniqu...\n",
            "\n",
            "--- Step: search_subtopic ---\n",
            "Input: {'query': '\"Comparative analysis of deep feature methods for discriminative object detection\"', 'subtopic': '\"Comparison of deep feature-based approaches for discriminative object localization\"', 'num_results': 2}\n",
            "Output: 2 items\n",
            "  1. {'title': 'Overview Applications Analysis Technolo...\n",
            "  2. {'title': 'Overview Studies Feature Technology Dee...\n",
            "\n",
            "--- Step: generate_summary ---\n",
            "Input: {'topic': 'Learning deep features for discriminative localization', 'num_results': 7}\n",
            "Output: The research topic \"Learning deep features for discriminative localization\" explores how deep featur...\n",
            "\n",
            "--- Step: critique_summary ---\n",
            "Input: {'summary': 'The research topic \"Learning deep features for discriminative localization\" explores how deep feature learning can enhance discriminative localization and classification. A key study, \"Deep Feature Learning For Discriminative Localization And Classification\" (1, 2, 3), emphasizes the significance of this approach for various aspects including localization and learning. Another notable work, \"Deep Learning Techniques For Feature Extraction In Discriminative Landmark Identification\" (1, 2), discusses deep learning techniques for feature extraction, highlighting their importance in discriminative landmark identification. The comparative analysis in \"Comparative Analysis Of Deep Feature Methods For Discriminative Object Detection\" (1, 2) examines deep feature methods for discriminative object detection, underlining their significance in this field. Overall, deep feature learning is a crucial method for improving discriminative localization and classification in various applications.', 'topic': 'Learning deep features for discriminative localization'}\n",
            "Output: Dictionary with keys ['strengths', 'weaknesses', 'suggestions', 'additional_topics']\n",
            "\n",
            "--- Step: improve_summary ---\n",
            "Input: {'original_summary': 'The research topic \"Learning deep features for discriminative localization\" explores how deep feature learning can enhance discriminative localization and classification. A key study, \"Deep Feature Learning For Discriminative Localization And Classification\" (1, 2, 3), emphasizes the significance of this approach for various aspects including localization and learning. Another notable work, \"Deep Learning Techniques For Feature Extraction In Discriminative Landmark Identification\" (1, 2), discusses deep learning techniques for feature extraction, highlighting their importance in discriminative landmark identification. The comparative analysis in \"Comparative Analysis Of Deep Feature Methods For Discriminative Object Detection\" (1, 2) examines deep feature methods for discriminative object detection, underlining their significance in this field. Overall, deep feature learning is a crucial method for improving discriminative localization and classification in various applications.', 'critique': {'strengths': ['The summary accurately identifies the main focus of the research topic: deep feature learning for discriminative localization and classification.', 'It includes specific studies to support the discussion and provide evidence for the importance of deep feature learning in this field.', 'The summary briefly mentions the significance of deep feature learning in various applications.'], 'weaknesses': ['The summary lacks clarity in explaining what deep feature learning is and how it enhances discriminative localization and classification.', \"There is no clear organization of ideas, making it difficult to follow the summary's progression.\", 'Potential biases or limitations are not addressed, such as the challenges or drawbacks of deep feature learning methods.'], 'suggestions': ['Begin with a clear definition of deep feature learning and its relevance to discriminative localization and classification.', 'Reorganize the summary to present ideas in a logical order, perhaps starting with the problem, followed by the solution (deep feature learning), and then supporting evidence from the studies.', 'Address potential biases or limitations, such as computational requirements, dependence on large datasets, or limitations in generalizability.'], 'additional_topics': ['Exploring real-world applications of deep feature learning for discriminative localization and classification.', 'Comparing deep feature learning methods with traditional feature extraction techniques and discussing their advantages and disadvantages.']}, 'topic': 'Learning deep features for discriminative localization'}\n",
            "Output: Deep feature learning is a crucial method that enhances discriminative localization and classificati...\n",
            "\n",
            "==================================================\n",
            "RESEARCH COMPLETE\n",
            "==================================================\n",
            "Topic: Learning deep features for discriminative localization\n",
            "\n",
            "Subtopics explored:\n",
            "- \"Deep learning methods for feature extraction in discriminative localization\"\n",
            "- \"Comparison of deep feature-based approaches for discriminative object localization\"\n",
            "\n",
            "Final Summary:\n",
            "Deep feature learning is a crucial method that enhances discriminative localization and classification by enabling the learning of more distinctive and informative representations. The research topic \"Learning deep features for discriminative localization\" emphasizes the significance of this approach through key studies such as \"Deep Feature Learning For Discriminative Localization And Classification,\" which highlights the advantages of deep features in various aspects including localization and learning. Additionally, \"Deep Learning Techniques For Feature Extraction In Discriminative Landmark Identification\" discusses the importance of deep learning techniques in feature extraction for discriminative landmark identification. However, it is important to acknowledge potential biases or limitations, such as the computational requirements and dependence on large datasets, associated with deep feature learning methods. Despite these challenges, deep feature learning remains a valuable tool for improving discriminative localization and classification in various applications.\n",
            "==================================================\n",
            "\n",
            "Show detailed research results? (y/n): n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "n4Spr9nqz7hb"
      }
    }
  ]
}